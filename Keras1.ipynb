{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLWeiCIOUEOX/J3+O6jkBm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/electriccheffer/CVLabs2022/blob/main/Keras1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QstCWDkqmp-O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code snipit imports the required libraries for the Lab.  "
      ],
      "metadata": {
        "id": "1HvqmQNVndE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IM3bAJYnmYL",
        "outputId": "9a285217-7717-4839-919d-91b16fc3d2b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  786M  100  786M    0     0   241M      0  0:00:03  0:00:03 --:--:--  241M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik7H0fD3nx94",
        "outputId": "5ab416e3-ed9c-40a0-deec-9a000097403a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kagglecatsanddogs_3367a.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q kagglecatsanddogs_3367a.zip"
      ],
      "metadata": {
        "id": "4F4u_yTNoX0_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aitQNiuopbp",
        "outputId": "8ec625a2-8793-4680-c9cf-71bd7698c01e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " kagglecatsanddogs_3367a.zip   PetImages        sample_data\n",
            "'MSR-LA - 3467.docx'\t      'readme[1].txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, we have imported the images of some cats and dogs.  Below, we are performing some filtering of the data using the os library from  Python.  We are checking to make sure that the first ten bytes are properly configured with the jfif.  "
      ],
      "metadata": {
        "id": "6l6xgWVEoHoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "num_skipped = 0\n",
        "for folder_name in (\"Cat\",\"Dog\"):\n",
        "  folder_path = os.path.join(\"PetImages\",folder_name)\n",
        "  for f_name in os.listdir(folder_path):\n",
        "    fpath = os.path.join(folder_path,f_name)\n",
        "    try:\n",
        "      fobj = open(fpath,\"rb\")\n",
        "      is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "    finally:\n",
        "      fobj.close()\n",
        "    \n",
        "    if not is_jfif:\n",
        "      num_skipped += 1 \n",
        "      os.remove(fpath)\n",
        "print(\"Deleted %d images \" % num_skipped)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybQWHBkFsYnw",
        "outputId": "60042e02-2bab-40d3-c050-720773c1823b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted 1590 images \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The os Python library is being used here to build our file paths which we then turn into a file object and use the peek function to examine the first ten bytes of the path and make sure that the header tells us that the image is a jfif.\n",
        "\n",
        "We are using tf.compat.as_bytes().  Here is a link to the relevant libray used:  https://www.tensorflow.org/api_docs/python/tf/compat\n",
        "\n",
        "We now move into generating the dataset:"
      ],
      "metadata": {
        "id": "XDudoGG-zPx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (180,180)\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      \"PetImages\",\n",
        "      validation_split=0.2,\n",
        "      subset=\"training\",\n",
        "      seed=1337,\n",
        "      image_size=image_size,\n",
        "      batch_size=batch_size,\n",
        "\n",
        "  )\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      \"PetImages\",\n",
        "      validation_split=0.2,\n",
        "      subset=\"validation\",\n",
        "      seed=1337,\n",
        "      image_size=image_size,\n",
        "      batch_size=batch_size\n",
        "\n",
        "\n",
        "  )"
      ],
      "metadata": {
        "id": "rnYTBc1J226s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are making two datasets in the above code using the tensorflow.keras.preprocessing module documentation can be found in the following link: https://keras.io/api/preprocessing/\n",
        "\n",
        "The preprocessing module allows us to move data from disk into tensorflow.  We give the function in each case the parameters: a directory, validation_split, subset, seed, image_size, and batch_size.  \n",
        "\n",
        "**validation_split**: says what fraction of the input data to into validation set using a floating point value. \n",
        "**subset**:names the subsets as training or validation \n",
        "**seed**: a seed value used to randomize or transform the images.\n",
        "\n",
        "We now move onto viewing the images.  "
      ],
      "metadata": {
        "id": "8MDBoXmp5jHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U0TXq0M9_TGd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}